{"pageProps":{"note":{"id":"kurgn324lwx7qqs0ef1ffxj","title":"Sklearn","desc":"","updated":1641417314286,"created":1641105063867,"stub":false,"isDir":false,"custom":{"stub":false,"isDir":false},"fname":"s.l.python.libs.sklearn","type":"note","vault":{"fsPath":"DevLog"},"contentHash":"9404c7dec3f56820fa7edd36b4550402","links":[],"anchors":{"kaggle-ml-course":{"type":"header","text":"Kaggle ML Course","value":"kaggle-ml-course","line":11,"column":0,"depth":2},"reading-and-looking-at-the-shape-of-the-data":{"type":"header","text":"Reading and looking at the shape of the data","value":"reading-and-looking-at-the-shape-of-the-data","line":13,"column":0,"depth":3},"define-the-model":{"type":"header","text":"Define the Model","value":"define-the-model","line":40,"column":0,"depth":3},"mean-absolute-error":{"type":"header","text":"Mean Absolute Error","value":"mean-absolute-error","line":57,"column":0,"depth":3},"split-data-into-test-and-validation-sets":{"type":"header","text":"Split data into test and validation sets","value":"split-data-into-test-and-validation-sets","line":66,"column":0,"depth":3},"be-careful-and-consider-the-effects-of-both-overunderfitting":{"type":"header","text":"Be careful and consider the effects of both over/underfitting","value":"be-careful-and-consider-the-effects-of-both-overunderfitting","line":86,"column":0,"depth":3},"fit-model-using-all-data":{"type":"header","text":"Fit Model Using All Data","value":"fit-model-using-all-data","line":142,"column":0,"depth":3}},"children":[],"parent":"uf16h268w8q6iuscgzf5xuw","data":{}},"body":"<h1 id=\"sklearn\"><a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#sklearn\"></a>Sklearn</h1>\n<h2 id=\"kaggle-ml-course\"><a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#kaggle-ml-course\"></a>Kaggle ML Course</h2>\n<h3 id=\"reading-and-looking-at-the-shape-of-the-data\"><a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#reading-and-looking-at-the-shape-of-the-data\"></a>Reading and looking at the shape of the data</h3>\n<pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> pandas <span class=\"token keyword\">as</span> pd\n\nmelbourne_file_path <span class=\"token operator\">=</span> <span class=\"token string\">'../input/melbourne-housing-snapshot/melb_data.csv'</span>\nmelbourne_data <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>read_csv<span class=\"token punctuation\">(</span>melbourne_file_path<span class=\"token punctuation\">)</span> \nmelbourne_data<span class=\"token punctuation\">.</span>columns\n<span class=\"token comment\"># The Melbourne data has some missing values (some houses for which some variables weren't recorded.)</span>\n<span class=\"token comment\"># We'll learn to handle missing values in a later tutorial.  </span>\n<span class=\"token comment\"># Your Iowa data doesn't have missing values in the columns you use. </span>\n<span class=\"token comment\"># So we will take the simplest option for now, and drop houses from our data. </span>\n<span class=\"token comment\"># Don't worry about this much for now, though the code is:</span>\n\n<span class=\"token comment\"># dropna drops missing values (think of na as \"not available\")</span>\nmelbourne_data <span class=\"token operator\">=</span> melbourne_data<span class=\"token punctuation\">.</span>dropna<span class=\"token punctuation\">(</span>axis<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># Seleting the target prediction</span>\ny <span class=\"token operator\">=</span> melbourne_data<span class=\"token punctuation\">.</span>Price\n<span class=\"token comment\"># Choosing features to help you predict the target</span>\nmelbourne_features <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'Rooms'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Bathroom'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Landsize'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Lattitude'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Longtitude'</span><span class=\"token punctuation\">]</span>\n<span class=\"token comment\"># Concention is that the data is called X</span>\nX <span class=\"token operator\">=</span> melbourne_data<span class=\"token punctuation\">[</span>melbourne_features<span class=\"token punctuation\">]</span>\nX<span class=\"token punctuation\">.</span>describe<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nX<span class=\"token punctuation\">.</span>head<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n</code></pre>\n<h3 id=\"define-the-model\"><a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#define-the-model\"></a>Define the Model</h3>\n<pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>tree <span class=\"token keyword\">import</span> DecisionTreeRegressor\n\n<span class=\"token comment\"># Define model. Specify a number for random_state to ensure same results each run</span>\nmelbourne_model <span class=\"token operator\">=</span> DecisionTreeRegressor<span class=\"token punctuation\">(</span>random_state<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Fit model</span>\nmelbourne_model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Making predictions for the following 5 houses:\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">.</span>head<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"The predictions are\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>melbourne_model<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">.</span>head<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n</code></pre>\n<h3 id=\"mean-absolute-error\"><a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#mean-absolute-error\"></a>Mean Absolute Error</h3>\n<pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>metrics <span class=\"token keyword\">import</span> mean_absolute_error\n\npredicted_home_prices <span class=\"token operator\">=</span> melbourne_model<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">)</span>\nmean_absolute_error<span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">,</span> predicted_home_prices<span class=\"token punctuation\">)</span>\n</code></pre>\n<h3 id=\"split-data-into-test-and-validation-sets\"><a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#split-data-into-test-and-validation-sets\"></a>Split data into test and validation sets</h3>\n<pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>model_selection <span class=\"token keyword\">import</span> train_test_split\n\n<span class=\"token comment\"># split data into training and validation data, for both features and target</span>\n<span class=\"token comment\"># The split is based on a random number generator. Supplying a numeric value to</span>\n<span class=\"token comment\"># the random_state argument guarantees we get the same split every time we</span>\n<span class=\"token comment\"># run this script.</span>\ntrain_X<span class=\"token punctuation\">,</span> val_X<span class=\"token punctuation\">,</span> train_y<span class=\"token punctuation\">,</span> val_y <span class=\"token operator\">=</span> train_test_split<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">,</span> random_state <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># Define model</span>\nmelbourne_model <span class=\"token operator\">=</span> DecisionTreeRegressor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># Fit model</span>\nmelbourne_model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>train_X<span class=\"token punctuation\">,</span> train_y<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># get predicted prices on validation data</span>\nval_predictions <span class=\"token operator\">=</span> melbourne_model<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>val_X<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>mean_absolute_error<span class=\"token punctuation\">(</span>val_y<span class=\"token punctuation\">,</span> val_predictions<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n</code></pre>\n<h3 id=\"be-careful-and-consider-the-effects-of-both-overunderfitting\"><a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#be-careful-and-consider-the-effects-of-both-overunderfitting\"></a>Be careful and consider the effects of both over/underfitting</h3>\n<pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Code you have previously used to load data</span>\n<span class=\"token keyword\">import</span> pandas <span class=\"token keyword\">as</span> pd\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>metrics <span class=\"token keyword\">import</span> mean_absolute_error\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>model_selection <span class=\"token keyword\">import</span> train_test_split\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>tree <span class=\"token keyword\">import</span> DecisionTreeRegressor\n\n\n<span class=\"token comment\"># Path of the file to read</span>\niowa_file_path <span class=\"token operator\">=</span> <span class=\"token string\">'../input/home-data-for-ml-course/train.csv'</span>\n\nhome_data <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>read_csv<span class=\"token punctuation\">(</span>iowa_file_path<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># Create target object and call it y</span>\ny <span class=\"token operator\">=</span> home_data<span class=\"token punctuation\">.</span>SalePrice\n<span class=\"token comment\"># Create X</span>\nfeatures <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'LotArea'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'YearBuilt'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'1stFlrSF'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'2ndFlrSF'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'FullBath'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'BedroomAbvGr'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'TotRmsAbvGrd'</span><span class=\"token punctuation\">]</span>\nX <span class=\"token operator\">=</span> home_data<span class=\"token punctuation\">[</span>features<span class=\"token punctuation\">]</span>\n\n<span class=\"token comment\"># Split into validation and training data</span>\ntrain_X<span class=\"token punctuation\">,</span> val_X<span class=\"token punctuation\">,</span> train_y<span class=\"token punctuation\">,</span> val_y <span class=\"token operator\">=</span> train_test_split<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">,</span> random_state<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Specify Model</span>\niowa_model <span class=\"token operator\">=</span> DecisionTreeRegressor<span class=\"token punctuation\">(</span>random_state<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># Fit Model</span>\niowa_model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>train_X<span class=\"token punctuation\">,</span> train_y<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Make validation predictions and calculate mean absolute error</span>\nval_predictions <span class=\"token operator\">=</span> iowa_model<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>val_X<span class=\"token punctuation\">)</span>\nval_mae <span class=\"token operator\">=</span> mean_absolute_error<span class=\"token punctuation\">(</span>val_predictions<span class=\"token punctuation\">,</span> val_y<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Validation MAE: {:,.0f}\"</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>val_mae<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Set up code checking</span>\n<span class=\"token keyword\">from</span> learntools<span class=\"token punctuation\">.</span>core <span class=\"token keyword\">import</span> binder\nbinder<span class=\"token punctuation\">.</span>bind<span class=\"token punctuation\">(</span><span class=\"token builtin\">globals</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">from</span> learntools<span class=\"token punctuation\">.</span>machine_learning<span class=\"token punctuation\">.</span>ex5 <span class=\"token keyword\">import</span> <span class=\"token operator\">*</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\nSetup complete\"</span><span class=\"token punctuation\">)</span>\n</code></pre>\n<pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">get_mae</span><span class=\"token punctuation\">(</span>max_leaf_nodes<span class=\"token punctuation\">,</span> train_X<span class=\"token punctuation\">,</span> val_X<span class=\"token punctuation\">,</span> train_y<span class=\"token punctuation\">,</span> val_y<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  model <span class=\"token operator\">=</span> DecisionTreeRegressor<span class=\"token punctuation\">(</span>max_leaf_nodes<span class=\"token operator\">=</span>max_leaf_nodes<span class=\"token punctuation\">,</span> random_state<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n\tmodel<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>train_X<span class=\"token punctuation\">,</span> train_y<span class=\"token punctuation\">)</span>\n\tpreds_val <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>val_X<span class=\"token punctuation\">)</span>\n\tmae <span class=\"token operator\">=</span> mean_absolute_error<span class=\"token punctuation\">(</span>val_y<span class=\"token punctuation\">,</span> preds_val<span class=\"token punctuation\">)</span>\n\t<span class=\"token keyword\">return</span><span class=\"token punctuation\">(</span>mae<span class=\"token punctuation\">)</span>\n</code></pre>\n<pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Here is a short solution with a dict comprehension.</span>\n<span class=\"token comment\"># The lesson gives an example of how to do this with an explicit loop.</span>\nscores <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span>leaf_size<span class=\"token punctuation\">:</span> get_mae<span class=\"token punctuation\">(</span>leaf_size<span class=\"token punctuation\">,</span> train_X<span class=\"token punctuation\">,</span> val_X<span class=\"token punctuation\">,</span> train_y<span class=\"token punctuation\">,</span> val_y<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> leaf_size <span class=\"token keyword\">in</span> candidate_max_leaf_nodes<span class=\"token punctuation\">}</span>\nbest_tree_size <span class=\"token operator\">=</span> <span class=\"token builtin\">min</span><span class=\"token punctuation\">(</span>scores<span class=\"token punctuation\">,</span> key<span class=\"token operator\">=</span>scores<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">)</span>\n</code></pre>\n<h3 id=\"fit-model-using-all-data\"><a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#fit-model-using-all-data\"></a>Fit Model Using All Data</h3>\n<pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Fit the model with best_tree_size. Fill in argument to make optimal size</span>\nfinal_model <span class=\"token operator\">=</span> DecisionTreeRegressor<span class=\"token punctuation\">(</span>max_leaf_nodes<span class=\"token operator\">=</span>best_tree_size<span class=\"token punctuation\">,</span> random_state<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># fit the final model</span>\nfinal_model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">)</span>\n</code></pre>\n<ul>\n<li><code>sklearn.metrics.mean_absolute_error</code></li>\n<li><code>sklearn.tree.DecisionTreeRegressor</code>\n<ul>\n<li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html</a></li>\n</ul>\n</li>\n<li><code>sklearn.ensemble.RandomForestRegressor</code></li>\n</ul>\n<pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> pandas <span class=\"token keyword\">as</span> pd\n\n<span class=\"token comment\"># Load data</span>\nmelbourne_file_path <span class=\"token operator\">=</span> <span class=\"token string\">'../input/melbourne-housing-snapshot/melb_data.csv'</span>\nmelbourne_data <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>read_csv<span class=\"token punctuation\">(</span>melbourne_file_path<span class=\"token punctuation\">)</span> \n<span class=\"token comment\"># Filter rows with missing values</span>\nmelbourne_data <span class=\"token operator\">=</span> melbourne_data<span class=\"token punctuation\">.</span>dropna<span class=\"token punctuation\">(</span>axis<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># Choose target and features</span>\ny <span class=\"token operator\">=</span> melbourne_data<span class=\"token punctuation\">.</span>Price\nmelbourne_features <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'Rooms'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Bathroom'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Landsize'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'BuildingArea'</span><span class=\"token punctuation\">,</span> \n\t\t\t\t\t  <span class=\"token string\">'YearBuilt'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Lattitude'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Longtitude'</span><span class=\"token punctuation\">]</span>\nX <span class=\"token operator\">=</span> melbourne_data<span class=\"token punctuation\">[</span>melbourne_features<span class=\"token punctuation\">]</span>\n\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>model_selection <span class=\"token keyword\">import</span> train_test_split\n\n<span class=\"token comment\"># split data into training and validation data, for both features and target</span>\n<span class=\"token comment\"># The split is based on a random number generator. Supplying a numeric value to</span>\n<span class=\"token comment\"># the random_state argument guarantees we get the same split every time we</span>\n<span class=\"token comment\"># run this script.</span>\ntrain_X<span class=\"token punctuation\">,</span> val_X<span class=\"token punctuation\">,</span> train_y<span class=\"token punctuation\">,</span> val_y <span class=\"token operator\">=</span> train_test_split<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">,</span>random_state <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n</code></pre>\n<pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>ensemble <span class=\"token keyword\">import</span> RandomForestRegressor\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>metrics <span class=\"token keyword\">import</span> mean_absolute_error\n\nforest_model <span class=\"token operator\">=</span> RandomForestRegressor<span class=\"token punctuation\">(</span>random_state<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\nforest_model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>train_X<span class=\"token punctuation\">,</span> train_y<span class=\"token punctuation\">)</span>\nmelb_preds <span class=\"token operator\">=</span> forest_model<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>val_X<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>mean_absolute_error<span class=\"token punctuation\">(</span>val_y<span class=\"token punctuation\">,</span> melb_preds<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n</code></pre>\n<ul>\n<li><code>sklearn.impute.SimpleImputer</code>\n<ul>\n<li>Inserting the mean value of the column into the <code>NA</code> values so that it can lend itself to modeling purposes still without generating outliers or skewing the probability curve with anything other than the mean</li>\n</ul>\n</li>\n</ul>\n<pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>impute <span class=\"token keyword\">import</span> SimpleImputer\n\n<span class=\"token comment\"># Imputation</span>\nmy_imputer <span class=\"token operator\">=</span> SimpleImputer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nimputed_X_train <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span>my_imputer<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nimputed_X_valid <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span>my_imputer<span class=\"token punctuation\">.</span>transform<span class=\"token punctuation\">(</span>X_valid<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Imputation removed column names; put them back</span>\nimputed_X_train<span class=\"token punctuation\">.</span>columns <span class=\"token operator\">=</span> X_train<span class=\"token punctuation\">.</span>columns\nimputed_X_valid<span class=\"token punctuation\">.</span>columns <span class=\"token operator\">=</span> X_valid<span class=\"token punctuation\">.</span>columns\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"MAE from Approach 2 (Imputation):\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>score_dataset<span class=\"token punctuation\">(</span>imputed_X_train<span class=\"token punctuation\">,</span> imputed_X_valid<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">,</span> y_valid<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n</code></pre>\n<ul>\n<li>Better yet than just imputing is to track which values were imputed by row/column reference so that you can see the model differences by including and excluding those values</li>\n</ul>\n<pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Make copy to avoid changing original data (when imputing)</span>\nX_train_plus <span class=\"token operator\">=</span> X_train<span class=\"token punctuation\">.</span>copy<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nX_valid_plus <span class=\"token operator\">=</span> X_valid<span class=\"token punctuation\">.</span>copy<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Make new columns indicating what will be imputed</span>\n<span class=\"token keyword\">for</span> col <span class=\"token keyword\">in</span> cols_with_missing<span class=\"token punctuation\">:</span>\n\tX_train_plus<span class=\"token punctuation\">[</span>col <span class=\"token operator\">+</span> <span class=\"token string\">'_was_missing'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> X_train_plus<span class=\"token punctuation\">[</span>col<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>isnull<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\tX_valid_plus<span class=\"token punctuation\">[</span>col <span class=\"token operator\">+</span> <span class=\"token string\">'_was_missing'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> X_valid_plus<span class=\"token punctuation\">[</span>col<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>isnull<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Imputation</span>\nmy_imputer <span class=\"token operator\">=</span> SimpleImputer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nimputed_X_train_plus <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span>my_imputer<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>X_train_plus<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nimputed_X_valid_plus <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span>my_imputer<span class=\"token punctuation\">.</span>transform<span class=\"token punctuation\">(</span>X_valid_plus<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Imputation removed column names; put them back</span>\nimputed_X_train_plus<span class=\"token punctuation\">.</span>columns <span class=\"token operator\">=</span> X_train_plus<span class=\"token punctuation\">.</span>columns\nimputed_X_valid_plus<span class=\"token punctuation\">.</span>columns <span class=\"token operator\">=</span> X_valid_plus<span class=\"token punctuation\">.</span>columns\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"MAE from Approach 3 (An Extension to Imputation):\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>score_dataset<span class=\"token punctuation\">(</span>imputed_X_train_plus<span class=\"token punctuation\">,</span> imputed_X_valid_plus<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">,</span> y_valid<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n</code></pre>\n<pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Shape of training data (num_rows, num_columns)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Number of missing values in each column of training data</span>\nmissing_val_count_by_column <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">.</span>isnull<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>missing_val_count_by_column<span class=\"token punctuation\">[</span>missing_val_count_by_column <span class=\"token operator\">></span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n</code></pre>","noteIndex":{"id":"root","title":"root","desc":"","updated":1641013093667,"created":1595961348801,"stub":false,"custom":{"stub":false,"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"DevLog"},"contentHash":"b0b26527a2962dbb8bd5fb9a53ad702f","links":[],"anchors":{},"children":["rzqe5mjn3q2zfscw1roxr20","Bl9NeJmftBQJyJA3X4a6u","inm1S24v2GcN3Quf7gzDb","fmx7cfcdzale0ezna61yq5z","q6tr8q7gahfijix2ktlhcu3","yah6spesmpstech2ue2k3lq","6pxwlugphnw6vm4t8kn0j5p","i17wi8y2hgivywe08h6q6it","1st35wikvph2aew8aana21u","mijr0wvj3qz0mt9pv9xo1jv","qaf8v120h0ffvbowysjxw50","sh4851li2rsrhx47wwsirgv","fwqjigvqyfxiyl3pbpjvgdw","pxdvvln974xhe8w0alh0hoy","n6yddb1smrac5ll3l1y6wbd","g5dllyqoqkenoiz3opalzu6","kbwt8ucy0yh6bo8fl0kv9iw","beykavbe22agsufmm03hu0c","2uvuqa3c15o5r4j7sqadqvr","yr6gzhx0bhzyec6f52y66vf","4no90tcdswtuwmjm0bxnetx"],"parent":null,"data":{},"body":"\nThe hyperfixated rabbit hole diving knowledge base that is my brain looking at technology.\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"randomNote":{},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"sterkere","visibility":"private"},{"fsPath":"Norsk","visibility":"private"},{"fsPath":"DevLog"}],"journal":{"dailyDomain":"log","name":"daily","dateFormat":"yyyy.MM.dd","addBehavior":"childOfCurrent"},"scratch":{"name":"scratch","dateFormat":"yyyy.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"todoIntegration":true,"name":"task","dateFormat":"yyyy.MM.dd","addBehavior":"asOwnDomain","statusSymbols":{"":" ","wip":"wip","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":true,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"dendronVersion":"0.95.1","enableFullHierarchyNoteTitle":false,"enableHandlebarTemplates":false,"templateHierarchy":"template","enableSmartRefs":false},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"theme":"custom","enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"assetsPrefix":"/DevLog","copyAssets":true,"siteHierarchies":["root"],"enableSiteLastModified":true,"siteRootDir":"docs","siteUrl":"https://{GITHUB_USERNAME}.github.io","enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"duplicateNoteBehavior":{"action":"useVault","payload":["DevLog"]},"writeStubs":false,"seo":{"title":"Dendron","description":"Personal knowledge space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"master","editViewMode":"tree"},"enablePrettyLinks":true,"enableTaskNotes":true,"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true}